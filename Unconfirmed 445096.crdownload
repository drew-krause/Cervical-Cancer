# -*- coding: utf-8 -*-
"""CAPSTONE 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14cebyCNBxzxtVJo5_G43dEay1ODbn931
"""

#Creating Coding Environment
import pandas as pd 
import numpy as np
from matplotlib import pyplot as plt
from scipy import stats
from google.colab import files
import seaborn as sns
import io
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.preprocessing import normalize
from scipy.stats import kruskal
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score, cross_validate
from sklearn import  metrics
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.dummy import  DummyClassifier
from sklearn.model_selection import GridSearchCV
from imblearn.over_sampling import SMOTE
from collections import Counter
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn import  preprocessing

"""#Background:

Cervic Cancer kills over 300,000 women each year around the world. With eary detection survival rates increase expodentially. Thanks to test such as 'pap smears' and  Schillers, cervic cancer deaths have been decreaseing.

However even with pap smears and Schillers test some cases still go undiagonosed, and a false positive result on a pap smear causes unnessacry procedures, expenses, and worry to the patient.  Pap smears lead to a 4 women in 1,000 going undiaganosed and 95 in 1,000 false positives(Grimes et al, 2020).

#GOAL:
Use pap smear and shiller test results along with medical history to limit false positives and undiagonesd cases.

# Data 
The data was obtained from kaggle: [LINK](https://www.kaggle.com/loveall/cervical-cancer-risk-classification)

There are 36 columns with 858 rows of data. While at first look no NaN data is present, there seems to be some odd values in the dataframe. After taking a look at the excell file, whoever entered the data used ?s to fill in missing values. These '?' were converted to nan and handeled approperately.
"""

#Uploading CSV file with Data 
upload_1 = files.upload()

#Creating data frame 
risk_df = pd.read_csv(io.BytesIO(upload_1['risk_factors_cervical_cancer.csv']))

risk_df.shape
'''36 columns of 858 people'''

risk_df.info()

risk_df.isna().sum()
''' While there are no values stored as null, from glancing over CSV i know 
there are at least a few values with ? s stored'''

risk_df.where(risk_df=='?')
#''' Converting ?s to  Nan '''

risk_df = risk_df.replace(to_replace='?', value= np.nan)
'''Counting number of Nana Values'''
risk_df.isnull().sum()/858*100

"""## Data Cleaning:



Dropping:

two columns, time since first and last diagnosis were dropped as 91% of data was missing

Mean:

Number of sexual partners, first sexual intercoure, and num of pregnancies will be filled via the mean


As there is the same 12% of data missing for several columns, these rows will be droped as it is the same data missing for all 





"""

#Converting from object to numeric
risk_df['Number of sexual partners'] = pd.to_numeric(risk_df['Number of sexual partners'])
risk_df['First sexual intercourse'] = pd.to_numeric(risk_df['First sexual intercourse'])
risk_df['Num of pregnancies'] = pd.to_numeric(risk_df['Num of pregnancies'])

#DROPING TWO 91% EMpty Columns 
risk_df = risk_df.drop(axis=1, labels=['STDs: Time since first diagnosis','STDs: Time since last diagnosis'])



risk_df['Number of sexual partners'] = risk_df['Number of sexual partners'].fillna(np.mean(risk_df['Number of sexual partners']))
risk_df['First sexual intercourse'] = risk_df['First sexual intercourse'].fillna(np.mean(risk_df['First sexual intercourse']))
risk_df['Num of pregnancies'] = risk_df['Num of pregnancies'].fillna(np.mean(risk_df['Num of pregnancies']))

risk_df.isna().sum()

risk_df_cl = risk_df.dropna()

risk_df_cl.info()
#after filling or droping all NA variables we have  726 rows to work with

li = []
df= pd.DataFrame()
for x in risk_df_cl:
  for y in risk_df_cl[x]:
     li.append(float(y))
  df[x] = li
  li=[]
for x in df:
  df[x] = df[x].astype(float)

print(df['Biopsy'].sum(),
risk_df['Biopsy'].sum(),)
Y = df.Biopsy
print(Y.sum())

"""## DATA EXPLORATION"""

df.corrwith(df['Biopsy']).sort_values(ascending= False)

df.corrwith(df.Schiller).sort_values(ascending = False)

df['Biopsy'].unique()

'''sns.pairplot(data = df, hue='Biopsy')'''

df.shape()

"""# Method:

Both Random Forrest and Gradient Boost algorithims were used to try and predict cancer cases using medical history and Pap smear tests. The data is unbalances and therefor the oversampling method SMOTE was used to improve predictions. A Dummy Algorithim achieved an accuracy of 93%, making 93% the bare minimum for any prediction method.

#Models
"""

ab = pd.DataFrame()
ab['Biospy'] = df['Biopsy'] 
df = df.drop(columns='Biopsy')
li = []
for x in df:
   li = df[x]
   li = preprocessing.normalize([li])
   df[x] = li.reshape(-1,1)
   li = []
df['Biopsy'] = ab["Biospy"]



"""##DUMMY

: A dummy model was used to see how the baseline accuracy compares to just selecting the major class. The use of a dummy model was implemented due to the heavy imbalance in the datasets.
"""

dum = DummyClassifier(strategy = 'most_frequent')
dum.fit(x_train,y_train)
dum.score(x_test,y_test)

"""## RANDOM FORREST

Intial random forrest model to see performance with no tunning:
"""

X = df.drop(axis = 1, labels = ['Biopsy',
                                'Schiller',
                                'Hinselmann',
                                'Citology'] )
x_test, x_train, y_test, y_train = train_test_split(X,Y)
rfc = RandomForestClassifier()
score_train = cross_val_score(rfc,x_train,y_train,cv=5)
score_test = cross_val_score(rfc,x_test,y_test,cv=5)
print(np.mean(score_train))
print(np.mean(score_test))

rfc_2 = RandomForestClassifier(n_estimators=1000, class_weight='balanced',ccp_alpha= .00005)
rfc_2.fit(x_train,y_train)
score_train = cross_val_score(rfc_2, x_train,y_train, cv = 5)
score_test = cross_val_score(rfc_2, x_test,y_test, cv = 5)
print(np.mean(score_train))
print(np.mean(score_test))

"""## Gradient Boosting

### Intial Model
"""

gbc = GradientBoostingClassifier(n_estimators=1000, learning_rate= .1, ccp_alpha= .005)
gbc.fit(x_train,y_train)
print(gbc.score(x_train,y_train))
print(gbc.score(x_test,y_test))
print(gbc.feature_importances_)

features = ['STDs: Number of diagnosis',    
            'Dx:Cancer',                                           
            'Smokes (years)',                         
            'Hormonal Contraceptives (years)',      
            'Num of pregnancies',                   
            'Age',                                
            'IUD',]
X_1 = df[features]
x_train2,x_test2, y_train2,y_test2 = train_test_split(X_1,Y)

gbc_2 = GradientBoostingClassifier(n_estimators=3000, min_samples_split= 4, learning_rate= .0001,ccp_alpha= .0001, max_depth=2, subsample=.8)
gbc_2.fit(x_train2,y_train2)
print(
gbc_2.score(x_train2,y_train2),
gbc_2.score(x_test2,y_test2))

gbc0 = GradientBoostingClassifier(random_state=8)
gbc0.fit(x_train2,y_train2)
y_pred = gbc0.predict(x_test2)
print( confusion_matrix(y_test2,y_pred, normalize='true'))
print(classification_report(y_test2, y_pred))

p_test2 = {'max_depth':[2,3,4,5,6,7] }
tuning = GridSearchCV(estimator =GradientBoostingClassifier(learning_rate=0.005,n_estimators=100, min_samples_split=2, min_samples_leaf=1, subsample=1,max_features='sqrt', random_state=10), 
            param_grid = p_test2, scoring='accuracy',n_jobs=4,iid=False, cv=5)
tuning.fit(x_train,y_train)
tuning.best_params_, tuning.best_score_

model1 = GradientBoostingClassifier(learning_rate=0.005, n_estimators=100,max_depth=2, min_samples_split=2, min_samples_leaf=1, subsample=1,max_features='sqrt', random_state=10)
model1.fit(x_train,y_train)
predictors=list(x_train)
feat_imp = pd.Series(model1.feature_importances_, predictors).sort_values(ascending=False)
feat_imp.plot(kind='bar', title='Importance of Features')
plt.ylabel('Feature Importance Score')
print('Accuracy of the GBM on test set: {:.3f}'.format(model1.score(x_test, y_test)))
pred=model1.predict(x_test)
print(classification_report(y_test, pred))

"""### Including Pap smears and Schiller (Citology test)

"""

features_1 = ['STDs: Number of diagnosis',    
            'Dx:Cancer',                                           
            'Smokes (years)',                         
            'Hormonal Contraceptives (years)',      
            'Num of pregnancies',                   
            'Age',                                
            'IUD',
            'Citology']
x_3 = df[features_1]
x_train3, x_test3,y_train3,y_test3 = train_test_split(x_3,Y)
oversample = SMOTE()
x_train_os, y_train_os = oversample.fit_resample(x_train3,y_train3)
print(Counter(y_train_os))
from imblearn.under_sampling import RandomUnderSampler


gbc1 = GradientBoostingClassifier(learning_rate=0.005, n_estimators=100,max_depth=2, min_samples_split=2, min_samples_leaf=1, subsample=1,max_features='sqrt', random_state=10)
gbc1.fit(x_train_os,y_train_os)
y_pred = gbc1.predict(x_test3)
print( confusion_matrix(y_test3,y_pred, normalize='true'))
print(classification_report(y_test3, y_pred))

p_test ={'learning_rate':[0.2,0.15,0.1, 0.25,], 'n_estimators':[50,100,250,]}
tuning = GridSearchCV(estimator=GradientBoostingClassifier(max_depth=5, min_samples_split=2, min_samples_leaf=1, subsample=.8,max_features='sqrt', random_state=10), 
            param_grid = p_test, scoring='accuracy',n_jobs=4,iid=False, cv=5)
tuning.fit(x_train_os,y_train_os)
tuning.best_estimator_, tuning.best_score_,tuning.best_params_

gbc1 = GradientBoostingClassifier(learning_rate=0.2, n_estimators=100, max_depth=2,)
gbc1.fit(x_train_os,y_train_os)
y_pred = gbc1.predict(x_test3)
print(classification_report(y_test3,y_pred))

p_test2 = {'max_depth':[2,3,4,5,6,7,1] }
tuning = GridSearchCV(estimator =GradientBoostingClassifier( learning_rate=0.15,n_estimators=100, min_samples_split=2, min_samples_leaf=1, subsample=1,max_features='sqrt', random_state=10), 
            param_grid = p_test2, scoring='accuracy',n_jobs=4,iid=False, cv=5)
tuning.fit(x_train_os,y_train_os)
tuning.best_params_, tuning.best_score_

p_test3 = {'subsample':[.1, .2,.3,.4,.5,.6,.7,.8,.9,.10] }
tuning = GridSearchCV(estimator =GradientBoostingClassifier(learning_rate=0.15,n_estimators=100, min_samples_split=2, min_samples_leaf=1,max_features='sqrt',max_depth=2, random_state=10), 
            param_grid = p_test3, scoring='accuracy',n_jobs=4,iid=False, cv=5)
tuning.fit(x_train_os,y_train_os)
tuning.best_params_, tuning.best_score_



# Initail Model
x_train4, x_test4,y_train4,y_test4 = train_test_split(pca_df,Y)
oversample = SMOTE()
x_train_os, y_train_os = oversample.fit_resample(x_train4,y_train4)
print(Counter(y_train_os))
gbc1 = GradientBoostingClassifier(learning_rate=0.005, n_estimators=100,max_depth=2, min_samples_split=2, min_samples_leaf=1, subsample=1,max_features='sqrt', random_state=10)
gbc1.fit(x_train_os,y_train_os)
y_pred = gbc1.predict(x_test4)
print( confusion_matrix(y_test4,y_pred, normalize='true'))
print(classification_report(y_test4, y_pred))

"""### Tunning Model"""

p_test ={'learning_rate':[0.2,0.15,0.1, 0.25,], 'n_estimators':[50,100,250,500, 1000, 2000]}
tuning = GridSearchCV(estimator=GradientBoostingClassifier(max_depth=2, min_samples_split=2, min_samples_leaf=1, subsample=.95,max_features='sqrt', random_state=10), 
            param_grid = p_test, scoring='accuracy',n_jobs=4,iid=False, cv=5)
tuning.fit(x_train_os,y_train_os)
tuning.best_estimator_, tuning.best_score_,tuning.best_params_

gbc1 = GradientBoostingClassifier(learning_rate=0.2, n_estimators=1000,max_depth=2, min_samples_split=2, min_samples_leaf=1, subsample=1,max_features='sqrt', random_state=10)
gbc1.fit(x_train_os,y_train_os)
y_pred = gbc1.predict(x_test4)
print( confusion_matrix(y_test4,y_pred, normalize='true'))
print(classification_report(y_test4, y_pred))

p_test2 = {'max_depth':[2,3,4,5,6,7,1] }
tuning = GridSearchCV(estimator =GradientBoostingClassifier( learning_rate=0.15,n_estimators=100, min_samples_split=2, min_samples_leaf=1, subsample=1,max_features='sqrt', random_state=10), 
            param_grid = p_test2, scoring='accuracy',n_jobs=4,iid=False, cv=5)
tuning.fit(x_train_os,y_train_os)
tuning.best_params_, tuning.best_score_

"""### All Features  (Excluding Hinselmann as it is majorly invasive)"""

# Initial Model 
feature = df.drop(axis= 1, columns=['Biopsy', 'Hinselmann']  )
x_train5, x_test5,y_train5,y_test5 = train_test_split(feature,Y)
oversample = SMOTE()
x_train_os, y_train_os = oversample.fit_resample(x_train5,y_train5)
print(Counter(y_train_os))
gbc1 = GradientBoostingClassifier(learning_rate=0.005, n_estimators=100,max_depth=2, min_samples_split=2, min_samples_leaf=1, subsample=1, random_state=10)
gbc1.fit(x_train_os,y_train_os)
y_pred = gbc1.predict(x_test5)
print( confusion_matrix(y_test5,y_pred, normalize='true'))
print(classification_report(y_test5, y_pred))

"""A recall of 1 indicatates there were no missed cases, however  a precision of 0.47  means that 53% of cases are a false positive"""

p_test ={'learning_rate':[1E-20,1E-10,1E-5,1E-1], 'n_estimators':[50,100,250,500, 1000, 2000]}
tuning = GridSearchCV(estimator=GradientBoostingClassifier(max_depth=3, min_samples_split=5, min_samples_leaf=1, subsample=1, random_state=10), 
            param_grid = p_test, scoring='accuracy',n_jobs=4,iid=False, cv=5)
tuning.fit(x_train_os,y_train_os)
tuning.best_estimator_, tuning.best_score_,tuning.best_params_

"""BEST MODEL BELOW """



gbc1 = GradientBoostingClassifier(learning_rate=.01 , n_estimators=1000,max_depth=3, min_samples_split=5, min_samples_leaf=1,subsample=1, random_state=10 )
gbc1.fit(x_train_os,y_train_os)
print(gbc1.score(x_train_os,y_train_os))
y_pred = gbc1.predict(x_test5)
print( confusion_matrix(y_test5,y_pred, normalize='true'))
print(classification_report(y_test5, y_pred))

gbc1 = GradientBoostingClassifier(learning_rate=.01 , n_estimators=1000,max_depth=3, min_samples_split=6, min_samples_leaf=1,subsample=.8, random_state=10 )
gbc1.fit(x_train_os,y_train_os)
print(gbc1.score(x_train_os,y_train_os))
y_pred = gbc1.predict(x_test5)
print( confusion_matrix(y_test5,y_pred, normalize='true'))
print(classification_report(y_test5, y_pred))

"""###FINAL MODEL"""

gbc1 = GradientBoostingClassifier(learning_rate=.01 , n_estimators=1000,max_depth=2, min_samples_split=6, min_samples_leaf=1,subsample=.8, random_state=10 )
gbc1.fit(x_train_os,y_train_os)
print(gbc1.score(x_train_os,y_train_os))
y_pred = gbc1.predict(x_test5)
print( confusion_matrix(y_test5,y_pred, normalize='true'))
print(classification_report(y_test5, y_pred))

"""# Results:

Using  the Gradient Boosting algorithim proided by Sklearn, and then tunning the parameters a prediction accuray of 97% was achieved. 

The algorithim did not miss any cancer cases, resulting in 0 false negatives.  The algorithim did  have false postivies. However, the algorithim had a false positive rate of 2.9%. A pap smear alone has a false postive rate of 10%. 

The algorithm reduced false positives by about 7%, significantly reducing the amount of unnesarcy colonoscophys. In addition to avoding surgery patiends are save time, money and worry.

# Recomendations

1) The data set was fairly limited with only a little more than 500 cases, a more extensive study with more data is required

2) Of the 500 cases only about 10% had cervical cancer. With only 50 positive cases, oversampleing can lead to the algorithm overfiting this data, even with splitting the data into a train and test set. 

3) Doctors should include basic medical background questions in their anaylsis of pap smear test. Using this algorithim they can get a more accurate assesment of their patients risk for cervical cancer
"""